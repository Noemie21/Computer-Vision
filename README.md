# ğŸ¯ Computer Vision Projects

![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c?logo=pytorch&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-4.x-5C3EE8?logo=opencv&logoColor=white)
![YOLO](https://img.shields.io/badge/YOLOv8-Ultralytics-00FFFF)
![CLIP](https://img.shields.io/badge/CLIP-OpenAI-412991)
![Polytechnique MontrÃ©al](https://img.shields.io/badge/Polytechnique_MontrÃ©al-INF6804-red)

A collection of computer vision projects exploring classical and deep learning approaches for image description, segmentation, and object tracking.

---

## ğŸ“š Projects Overview

| Project | Topic | Methods Compared | Key Findings |
|---------|-------|------------------|--------------|
| [**01_ROI_Description**](./01_ROI_Description/) | Image Feature Extraction | HOG vs CLIP | CLIP 100% Top-5 accuracy, HOG 2.7Ã— faster |
| [**02_Video_Segmentation**](./02_Video_Segmentation/) | Semantic Segmentation | CLIPseg vs YOLOv8 | YOLO 26Ã— better IoU, CLIPseg wins zero-shot |
| [**03_Object_Tracking**](./03_Object_Tracking/) | Multi-Object Tracking | DeepSORT + YOLOv8 | 82.4% LocA, 38.8% HOTA on MOT17 |

---

## ğŸ—ï¸ Repository Structure

```
Computer-Vision/
â”‚
â”œâ”€â”€ 01_ROI_Description/          # HOG vs CLIP feature extraction
â”‚   â”œâ”€â”€ HOG.ipynb
â”‚   â”œâ”€â”€ CLIP.ipynb
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ 02_Video_Segmentation/       # CLIPseg vs YOLO segmentation
â”‚   â”œâ”€â”€ CLIPseg.ipynb
â”‚   â”œâ”€â”€ YOLO.ipynb
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ 03_Object_Tracking/          # DeepSORT multi-object tracking
â”‚   â”œâ”€â”€ YOLO_Track.ipynb
â”‚   â”œâ”€â”€ results.txt
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ README.md                    # This file
```

---

## ğŸ”¬ Methods & Technologies

### Classical Computer Vision
- **HOG (Histogram of Oriented Gradients)** â€” Handcrafted feature descriptor using gradient orientations

### Deep Learning / Vision-Language Models
- **CLIP** â€” OpenAI's contrastive vision-language model for zero-shot classification
- **CLIPseg** â€” CLIP-based semantic segmentation with text prompts
- **YOLOv8** â€” State-of-the-art real-time object detection and segmentation
- **DeepSORT** â€” Deep learning-enhanced multi-object tracking with appearance descriptors

### Frameworks & Libraries
```
torch, torchvision          # Deep learning
ultralytics                 # YOLOv8
transformers                # CLIP, CLIPseg
scikit-image                # HOG implementation
opencv-python               # Image processing
scipy                       # Distance metrics
TrackEval                   # MOT benchmark evaluation
```

---

## ğŸ“Š Key Results Summary

### 01 â€” Feature Extraction (HOG vs CLIP)
| Metric | HOG | CLIP |
|--------|-----|------|
| Execution Speed | **2.7Ã— faster** | 1Ã— |
| Face Recognition (avg distance) | 0.314 | **0.073** |
| Top-5 Accuracy | ~60% | **100%** |

### 02 â€” Segmentation (CLIPseg vs YOLO)
| Test Case | Winner |
|-----------|--------|
| Zero-shot (rare animals) | **CLIPseg** |
| Cluttered scenes | **CLIPseg** |
| Small objects | **CLIPseg** |
| IoU Precision | **YOLO** (26Ã— higher) |
| Speed | **YOLO** (1.72Ã— faster) |
| Occlusion handling | **YOLO** |

### 03 â€” Object Tracking (DeepSORT)
| Metric | Score | Description |
|--------|-------|-------------|
| LocA | 82.4% | Bounding box localization |
| AssA | 45.2% | ID association over time |
| DetA | 33.5% | Detection accuracy |
| HOTA | 38.8% | Overall tracking quality |

---

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/Noemie21/Computer-Vision.git
cd Computer-Vision

# Install dependencies
pip install torch torchvision ultralytics transformers scikit-image opencv-python scipy

# Run any notebook
jupyter notebook 01_ROI_Description/CLIP.ipynb
```

---


## ğŸ“„ License

This project is part of academic coursework at Polytechnique MontrÃ©al.

---

## ğŸ”— References

- [CLIP Paper (Radford et al., 2021)](https://arxiv.org/abs/2103.00020)
- [CLIPseg Paper (LÃ¼ddecke & Ecker, 2022)](https://arxiv.org/abs/2112.10003)
- [Ultralytics YOLOv8](https://docs.ultralytics.com)
- [DeepSORT](https://github.com/nwojke/deep_sort)
- [HOTA Metric (Luiten et al., 2021)](https://arxiv.org/abs/2009.07736)
